# 분류 목적을 위한 머신러닝
분류기법은 설명하고자 하는 목적변수(혹은 반응변수)가 이산형이나 명목형 형태의 특정
속성 카테고리로 구분할 수 있는 경우 사용되는 기법으로서 머신러닝 기반 데이터 분석에
서 가장 일반적이고 자주 접하게 되는 문제라고 할 수 있다.

# 분류목적 머신러닝 알고리즘 종류

![2018 03 20 13 52 16 001](https://user-images.githubusercontent.com/37295363/37636566-406e5ebe-2c46-11e8-8de3-df997d0c13b9.png)
![2018 03 20 13 52 26 001](https://user-images.githubusercontent.com/37295363/37636524-fde05746-2c45-11e8-897e-132193ed3d21.png)


## 1. K-최근접 이웃(K-Nearest Neighbor) 기법
K-최근접 이웃 기법은 목표변수의 범주를 알지 못하는 데이터 세트의 분류를 위해 해당
데이터 세트와 가장 유사한 주변 데이터 세트의 범주로 지정하는 방식으로 분류예측을 하
는 기법이다. 이러한 K-최근접 이웃 기법에서는 해당 데이터 점과 주변 데이터 세트 간의
‘유사성’을 어떻게 측정할 것인가와 최종적으로 목표변수의 범주를 분류할 때 주변 데
이터 세트 몇 개를 기준으로 판단할 것인가에 대한 기준이 필요하다.

### (1) 유사성 측정 방법
데이터 간의 유사성을 측정하는 방식은 여러 가지가 있지만, 일반적으로 두 점간의 유
클리디안 제곱 거리의 역수를 취하거나 피어슨 상관계수를 이용하여 유사성을 계산한
다. 점들이 이산형 변수일 경우 자카드 계수(Jaccard coefficient)를 사용한다.

### (2) 목표변수 분류 기준
K-최근접 이웃에서의 ‘K’는 해당 데이터 점과 주변 데이터 세트 간의 유사성을 측
정한 후, 해당 데이터 점의 목표변수 분류를 위해 참조할 주변 데이터 점들의 개수를
의미한다. 예를 들어 어떤 고객이 좋아하는 영화와 유사하면서 기존에 시청하지 않았
던 영화에 대한 추천을 생각해볼 수 있다. 이럴 때 해당 고객이 좋아하는 영화와 가장
유사한 영화 1개만 고려한다면 ‘1-근접이웃’이라고 말할 수 있고, 유사한 영화 3개
를 고려한다면 ‘3-근접이웃’이라고 말할 수 있다. 이런 식으로 해당 데이터 점과 유
사한 k개의 주변 데이터 점을 살펴보고, 해당 데이터 점의 분류범주를 고려하여 다수
결의 원칙에 따라 새로운 범주를 결정하는 방식이 K-근접이웃 기법이다.

## 2. 나이브 베이즈(Naive Bayes) 기법
나이브 베이즈 기법은 목표변수의 범주를 학습시키기 위해 통계학에서 사용되는 베이즈
정리를 사용한다. 즉, 나이브 베이즈 기법은 베이즈 확률 추정에 기반을 둔 확률모형이다.
이러한 나이브 베이즈 기법을 적용하기 위해서는 베이즈 정리와 사후확률의 개념을 이해
할 필요가 있다.

### (1) 베이즈 정리(Bayes’theorem)
베이즈 정리는 특정 사건이 발생할 확률을 다음과 같은 형태로 표현한 이론을 말한다.
즉, 사건 B가 일어났을 때 사건 A가 일어날 확률(사후확률)은 사건 A가 일어날 확률
(사전확률)과 사건 A가 일어났을 때 사건 B가 일어날 조건부 확률 P(B|A)의 곱을 사건
B가 일어날 확률 P(B)로 나누어 알아낼 수 있다는 뜻이다. 특히 분모에 있는 사건 B의
확률 P(B)는 다음과 같이 주변 확률(Marginal Probability)로 나타낼 수 있다.
베이즈 정리는 사전확률이 주어지고, 이에 따른 조건부확률 및 주변확률을 통
해 조건과 결과를 서로 바꿔서 사후확률을 계산하는 공식이라고 할 수 있다.

### (2) 나이브 베이즈 알고리즘
위나이브 베이즈 알고리즘을 다음과 같이 적용할 수
있다. 예를 들어 받은 이메일이 스팸일 확률을 구하는 문제를 고려해 보자.
실제 데이터를 관측하기 전에 경험이나 주관적 확신 등에 의거하여, ‘내가 오늘 받은
이메일 중 스팸 메일이 포함되어 있을 가능성은 대략 10%일 것 같다.’고 가정할 수
있다. 이렇게 추가적인 증거 없이 가능성을 추정하는 것을 ‘사전확률’이라고 할 수
있다. 이제 실제 데이터를 관측했다고 가정해보자. 오늘 도착한 이메일 중 스팸 메일
들에 ‘반짝할인’이라는 단어가 들어가 있는 조건부 확률 P(반짝할인|스팸) 및 주변확
률 P(반짝할인)의 확률을 계산할 수 있다. 이 경우 조건부확률 P(반짝할인|스팸)를 0.4,
주변확률 P(반짝할인)를 0.05라고 가정하면, 본 예시에서 구하고자 하는 사후확률 P(스
팸|반짝할인)는 (0.4*0.1)/0.05 = 0.8로 계산할 수 있다. 즉, 받은 이메일에서 ‘반짝할
인’이라는 단어가 들어가 있으면, 해당 이메일이 ‘스팸’일 확률은 80%라고 할 수
있다. 이는 데이터 세트를 관측하기 전의 사전확률 10%보다 8배 높아진 것이므로,
‘반짝할인’이라는 단어가 들어가 있는 이메일은 스팸 메일함으로 분류되어야 한다
는 ‘강한 확신’을 줄 수 있다고 볼 수 있다.

비록 단순한 예시지만, 매우 많은 단어 출현빈도들을 동시에 고려하면 바로 현실문제
에서 활용하는 스팸메일 필터의 원리와 거의 비슷한 분류기를 구현할 수 있다. 여기서
매우 많은 단어 빈도를 동시에 고려해야 하기 때문에, 모든 단어들이 출현할 가능성이
독립적이라고 가정하게 된다. 물론 이는 현실적이지 않다. 그렇기 때문에 이 기법이
‘Naive’라고 불리는 것이다. 그러나 실제로는 이렇게 독립성 가정이 유효하지 않다
고 판단될 때에도 실제로 나이브 베이즈 추정 결과는 여전히 결과를 잘 예측하는 것
으로 알려져 있다. 그것이 가능한 이유는 나이브 베이즈에서는 해당 목표변수의 범주
를 분류해내는 것이 목표인 것이지, 해당 분류에 속할 확률값의 정교함이 목적이 아니
기 때문이다. 즉, 스팸메일 판정 규칙이 “사후 확률값이 50% 이상일 때 스팸으로 판
정한다.” 일 경우 해당 확률 예측이 60%로 나왔든, 90%로 나왔든 모두 50% 이상이므
로 결과적으로 두 경우 모두 스팸으로 분류해버리기 때문에 예측 확률값 자체가 정확
하지 않아도 결과예측에는 큰 영향을 주지 않는다.

## 3. 로지스틱 회귀분석
로지스틱 회귀분석은 일반적인 선형회귀분석이 수치예측을 위해 활용되는 것과는 달리,
예측하고자 하는 목표변수(y)의 범주를 분류하고자 할 때 사용한다. 즉, 예측하고자 하는
것이 목표변수 Y 값이 아니라, 목표변수 Y가 특정 범주(i)가 될 확률 P(Y=i)이다.

## 4. 의사결정트리 기법
의사결정트리는 목표변수의 분류나 예측에 영향을 미치는 독립변수(설명변수 혹은 입력변
수)들의 속성 기준값에 따라 트리구조의 형태로 뿌리 노드부터 잎(리프) 노드까지 뻗어 나
가며 모델링을 하는 기법을 말한다. 의사결정트리는 설명변수의 특징이나 기준값에 따라
각 노드가 if-then의 형태로 분기되므로, 이런 트리구조를 따라감으로써, 각 데이터의 속
성값이 주어졌을 때 어떠한 카테고리로 분류되는지 쉽게 파악할 수 있다. 다음의 그림
(3.3)이 의사결정트리의 전형적인 모습을 보여준다. 아래의 예시에서 ‘나이’라고 되어
있는 원 모형의 노드가 뿌리(루트) 노드이며, 이는 대출실행 여부를 판별하는 데 있어서
‘나이’가 가장 유의한 변수라는 의미로 해석이 된다. 또한, 각 가지의 제일 마지막에 있
는 사각형 형태로 된 노드가 잎(리프) 노드이다.

## 5. 인공 신경망 분석
인공 신경망 분석 모델은 생물체의 뇌가 감각 입력 자극에 어떻게 반응하는지에 대한 이
해로부터 얻은 힌트를 바탕으로 생물체의 신경망을 모사하여, 입력 신호와 출력 신호 간
의 관계를 모델화하는 기법이다. 뇌가 뉴런이라는 세포들의 방대한 연결을 통해 신호를
39
처리하듯, 인공 신경망은 이를 모사한 인공 뉴런(노드)의 네트워크를 구성하여 모델화한
다. 아래의 그림 (3.4)는 생물체의 신경망과 인공 신경망 구조를 비교한 그림이다.

## 6. 서포트 벡터 머신(Support Vector Machine)
서포트 벡터(혹은 지지 벡터)머신은 서로 다른 분류에 속한 데이터 간에 간격(마진)이 최
대가 되는 선(또는 초평면)을 찾아서 이를 기준으로 데이터를 분류하는 모델이다.

### (1) 서포트 벡터 머신 모델
서포트 벡터 머신 모델은 아래의 그림 (3-6)에서 보는 바와 같이 두 범주 간의 데이터
를 최대로 나눌 최대 마진 초평면을 찾아서 각 데이터를 분류한다. 두 범주 간의 데이
터를 나누는 직선 혹은 평면은 여러 개가 있을 수 있지만, 현재의 훈련 데이터에서와
는 달리 평가용 데이터나 아직 알려지지 않은 미래의 데이터에서는 경계선 주변에 있
는 점들이 약간 변동됨으로써 분류 경계선을 반대편으로 넘어가는 점들이 생길 수 있
는 것이다. 이런 가능성을 최소로 하기 위해서 데이터 범주 간의 마진이 최대인 직선
(혹은 초평면)을 찾고자 하는 것이다. 즉 현재의 훈련 데이터가 아닌, 미래의 데이터를
분류 예측하는데 최대한 일반화하게 최대 구별을 이끌어 낼 수 있는 초평면을 찾고자
하는 것이다. 여기서 이 경계선과 가장 가까운 각 분류에 속한 점들을 서포트(혹은 지
지) 벡터(Support Vector)라고 하며, 각 분류는 최소 하나 이상의 서포트(지지) 벡터를
가지고 있어야 한다.

## 7. 랜덤 포레스트(Random Forest)
랜덤 포레스트는 의사결정트리 분석의 예측 정확도를 높이기 위해, 하나의 의사결정트리
를 사용하는 대신에 다수의 의사결정트리 집합을 사용하여 결과를 예측하는 앙상블
(Ensemble) 학습 기법이다. 앙상블 학습이란 보다 정확한 예측력을 가진 모델을 만들기
위해 여러 개의 모델을 학습한 다음, 예측 시 여러 모델의 예측 결과를 종합해서 결과를
도출하는 머신러닝 기법을 말한다. 일반적으로 앙상블 기법은 단일 모델에 의한 결과보다
일반화와 예측의 정확도가 높아지는 것으로 알려져 있다. 특히 랜덤 포레스트 기법은 훈
련 데이터 세트에서 임의의 샘플을 복원 추출하여, 해당 데이터에 대해서만 의사결정트리
를 만들어낸 뒤 이를 수백 개의 의사결정트리 모델에서 반복하여 결과를 계산한다. 새로
운 데이터에 대한 목표 변수 범주 분류의 예측을 수행할 때는 여러 개의 의사결정트리 모
델이 도출한 예측 결과를 투표 방식으로 결합한 뒤 과반수 이상인 범주로 결정하게 된다.
(분류 문제가 아닌 회귀 등의 수치 예측일 경우 목표변수 예측값을 평균 내고 그 평균값
을 최종 결과로 리턴하게 된다.) 이렇게 도출된 결과는 단일 의사결정트리보다 예측 성능
과 일반화 능력이 좋은 것으로 알려져 있다.
